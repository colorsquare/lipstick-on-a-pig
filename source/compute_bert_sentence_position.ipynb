{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (4.18.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from transformers) (4.8.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: dataclasses in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: sacremoses in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: filelock in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from transformers) (3.4.1)\n",
      "Requirement already satisfied: requests in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from tqdm>=4.27->transformers) (5.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: six in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from sacremoses->transformers) (1.1.1)\n",
      "Requirement already satisfied: click in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (1.10.2)\n",
      "Requirement already satisfied: typing_extensions in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: dataclasses in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from torch) (0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pickle5 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (0.0.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mpld3 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (0.5.8)\n",
      "Requirement already satisfied: jinja2 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from mpld3) (3.0.3)\n",
      "Requirement already satisfied: matplotlib in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from mpld3) (3.3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from jinja2->mpld3) (2.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from matplotlib->mpld3) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from matplotlib->mpld3) (8.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from matplotlib->mpld3) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from matplotlib->mpld3) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/team4/.local/lib/python3.6/site-packages (from matplotlib->mpld3) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from matplotlib->mpld3) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib->mpld3) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from scikit-learn) (1.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from scikit-learn) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pattern3 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (3.0.0)\n",
      "Requirement already satisfied: feedparser in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from pattern3) (6.0.10)\n",
      "Requirement already satisfied: docx in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from pattern3) (0.2.4)\n",
      "Requirement already satisfied: pdfminer3k in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from pattern3) (1.3.4)\n",
      "Requirement already satisfied: pdfminer.six in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from pattern3) (20221105)\n",
      "Requirement already satisfied: simplejson in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from pattern3) (3.18.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from pattern3) (4.11.1)\n",
      "Requirement already satisfied: cherrypy in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from pattern3) (18.8.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from beautifulsoup4->pattern3) (2.3.2.post1)\n",
      "Requirement already satisfied: portend>=2.1.1 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from cherrypy->pattern3) (3.0.0)\n",
      "Requirement already satisfied: zc.lockfile in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from cherrypy->pattern3) (2.0)\n",
      "Requirement already satisfied: more-itertools in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from cherrypy->pattern3) (8.14.0)\n",
      "Requirement already satisfied: cheroot>=8.2.1 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from cherrypy->pattern3) (9.0.0)\n",
      "Requirement already satisfied: jaraco.collections in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from cherrypy->pattern3) (3.4.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from cheroot>=8.2.1->cherrypy->pattern3) (1.16.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from cheroot>=8.2.1->cherrypy->pattern3) (4.8.3)\n",
      "Requirement already satisfied: jaraco.functools in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from cheroot>=8.2.1->cherrypy->pattern3) (3.4.0)\n",
      "Requirement already satisfied: tempora>=1.8 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from portend>=2.1.1->cherrypy->pattern3) (4.1.2)\n",
      "Requirement already satisfied: pytz in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2022.6)\n",
      "Requirement already satisfied: Pillow>=2.0 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from docx->pattern3) (8.4.0)\n",
      "Requirement already satisfied: lxml in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from docx->pattern3) (4.9.1)\n",
      "Requirement already satisfied: sgmllib3k in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from feedparser->pattern3) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from importlib-metadata->cheroot>=8.2.1->cherrypy->pattern3) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from importlib-metadata->cheroot>=8.2.1->cherrypy->pattern3) (3.6.0)\n",
      "Requirement already satisfied: jaraco.classes in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from jaraco.collections->cherrypy->pattern3) (3.2.1)\n",
      "Requirement already satisfied: jaraco.text in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from jaraco.collections->cherrypy->pattern3) (3.7.0)\n",
      "Requirement already satisfied: jaraco.context>=4.1 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern3) (4.1.1)\n",
      "Requirement already satisfied: importlib-resources in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern3) (5.4.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from pdfminer.six->pattern3) (38.0.4)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from pdfminer.six->pattern3) (2.0.12)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from cryptography>=36.0.0->pdfminer.six->pattern3) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern3) (2.21)\n",
      "Requirement already satisfied: ply in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from pdfminer3k->pattern3) (3.11)\n",
      "Requirement already satisfied: setuptools in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from zc.lockfile->cherrypy->pattern3) (58.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (3.6.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: click in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from click->nltk) (4.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from importlib-metadata->click->nltk) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from importlib-metadata->click->nltk) (3.6.0)\n",
      "Requirement already satisfied: importlib-resources in /home/team4/anaconda3/envs/exp/lib/python3.6/site-packages (from tqdm->nltk) (5.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install pickle5\n",
    "%pip install mpld3\n",
    "%pip install scikit-learn\n",
    "%pip install pattern3\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "\n",
    "def dump_pklfile(file, filepath, size):\n",
    "\twith open(filepath, \"wb\") as f:\n",
    "\t\tif (size == 0):\n",
    "\t\t\tpickle.dump((file), f)\n",
    "\t\t\treturn\n",
    "\t\tif (size > 0):\n",
    "\t\t\tpickle.dump((file[:size]), f)\n",
    "\t\t\treturn\n",
    "\t\telse:\n",
    "\t\t\tpickle.dump((file[size:]), f)\n",
    "\t\t\treturn\n",
    "    \n",
    "def open_pklfile(filepath, size):\n",
    "\twith open(filepath, \"rb\") as f:\n",
    "\t\tif (size == 0):\n",
    "\t\t\treturn pickle.load(f)\n",
    "\t\treturn (pickle.load(f))[0:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGULAR_NOUN_TEMPLATES = (\n",
    "    'This is {article} {term}.',\n",
    "    'That is {article} {term}.',\n",
    "    'There is {article} {term}.',\n",
    "    'Here is {article} {term}.',\n",
    "    'The {term} is here.',\n",
    "    'The {term} is there.',\n",
    ")\n",
    "\n",
    "PLURAL_NOUN_TEMPLATES = (\n",
    "    'These are {term}.',\n",
    "    'Those are {term}.',\n",
    "    'They are {term}.',\n",
    "    'The {term} are here.',\n",
    "    'The {term} are there.',\n",
    ")\n",
    "\n",
    "TARGET_POSITON=[(3, -2), (3, -2), (3, -2), (3, -2), (2, -4), (2, -4), (3, -2), (3, -2), (3, -2), (2, -4), (2, -4)]\n",
    "\n",
    "\n",
    "def fill_template(template, term):\n",
    "    article = (\n",
    "        'an'\n",
    "        if (\n",
    "            (\n",
    "                term.startswith('honor') or any(\n",
    "                    term.startswith(c) for c in 'aeiouAEIOU'\n",
    "                )\n",
    "            ) and not (\n",
    "                term.startswith('European') or term.startswith('Ukrainian')\n",
    "            )\n",
    "        )\n",
    "        else 'a'\n",
    "    )\n",
    "    sentence = template.format(article=article, term=term)\n",
    "    return sentence[0].upper() + sentence[1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pattern.en import pluralize, singularize\n",
    "\n",
    "def generate_noun_sentences(vocab):\n",
    "    tags = [(word, tag) for w in vocab for word, tag in nltk.pos_tag([w])]\n",
    "    nouns = [word for word, tag in tqdm(tags) if tag.startswith(\"N\") and '___' not in word and word is not \"_\"]\n",
    "    w2i = {w: i for i, w in enumerate(nouns)}\n",
    "\n",
    "    sentence_list = []\n",
    "    for term in tqdm(nouns):\n",
    "        singular_term = singularize(term)\n",
    "        sentences = []\n",
    "        sentences += [fill_template(template, singular_term) for template in SINGULAR_NOUN_TEMPLATES]\n",
    "        plurar_term = pluralize(term)\n",
    "        sentences += [fill_template(template, plurar_term) for template in PLURAL_NOUN_TEMPLATES]\n",
    "        sentence_list.append(sentences)\n",
    "    return w2i, nouns, sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained Bert model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/team4/anaconda3/envs/exp/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "debiased_model = \"bert\"\n",
    "folder_num = \"1.\"\n",
    "save_path = f\"../data/extracted_positional/{folder_num} {debiased_model}/{debiased_model}_sentence_\"\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open file and extract bert embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "from tqdm import tqdm \n",
    "import pickle5 as pickle\n",
    "\n",
    "def dump_pklfile(file, filepath, size):\n",
    "\twith open(filepath, \"wb\") as f:\n",
    "\t\tif (size == 0):\n",
    "\t\t\tpickle.dump((file), f)\n",
    "\t\t\treturn\n",
    "\t\tif (size > 0):\n",
    "\t\t\tpickle.dump((file[:size]), f)\n",
    "\t\t\treturn\n",
    "\t\telse:\n",
    "\t\t\tpickle.dump((file[size:]), f)\n",
    "\t\t\treturn\n",
    "    \n",
    "def open_pklfile(filepath, size):\n",
    "\twith open(filepath, \"rb\") as f:\n",
    "\t\tif (size == 0):\n",
    "\t\t\treturn pickle.load(f)\n",
    "\t\treturn (pickle.load(f))[0:size]\n",
    "\n",
    "gender_target_list=[\"male\", \"males\", \"man\", \"men\", \"boy\", \"boys\", \"brother\", \"brothers\", \"his\", \"he\", \"him\", \"son\", \"sons\",\n",
    "              \"female\", \"females\", \"woman\", \"women\", \"girl\", \"girls\", \"sister\",\"sisters\", \"she\", \"her\",\"hers\", \"daughter\", \"daughters\"]\n",
    "\n",
    "def extract_bert_embeddings(sentence_list):\n",
    "\t#init for stacking embeddings\n",
    "\tembeddings = torch.empty(0, device=device)\n",
    "\n",
    "\tfor sentences in tqdm(sentence_list):\n",
    "\t\tembedding = torch.empty(0, device=device)\n",
    "\t\tfor i, sentence in enumerate(sentences):\n",
    "\t\t\t# print(f\"[{sentence}]\")\n",
    "\t\t\t# Map the token strings to their vocabulary indeces.\n",
    "\t\t\tmarked_text = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "\t\t\t# print(marked_text)\n",
    "\t\t\ttokenized_text = tokenizer.tokenize(marked_text)\n",
    "\t\t\t# print(tokenized_text)\n",
    "\t\t\t# handling such as \"wedding_dress\"\n",
    "\t\t\ttokenized_text = [token for token in tokenized_text if token != '_']\n",
    "\n",
    "\t\t\t# Split the sentence into tokens.\n",
    "\t\t\tindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\t\t\tsegments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "\t\t\t# Convert inputs to PyTorch tensors\n",
    "\t\t\ttokens_tensor = torch.tensor([indexed_tokens], device=device)\n",
    "\t\t\tsegments_tensors = torch.tensor([segments_ids], device=device)\n",
    "\n",
    "\t\t\t# Put the model in \"evaluation\" mode,meaning feed-forward operation.\n",
    "\t\t\tmodel.eval()\n",
    "\n",
    "\t\t\t# Run the text through BERT, get the output and collect all of the hidden states produced from all 12 layers.\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\toutputs = model(tokens_tensor, segments_tensors).hidden_states\n",
    "\t\t\t\t# print(f\"outputs: {outputs}\")\n",
    "\t\t\t\tstart_position=TARGET_POSITON[i][0]\n",
    "\t\t\t\tend_position=TARGET_POSITON[i][1]\n",
    "\t\t\t\tlast_four_hidden_states = outputs[-4:]\n",
    "\t\t\t\tconcated_hidden_states = torch.cat(last_four_hidden_states, dim=2)\n",
    "\t\t\t\tsentence_embedding=torch.sum(torch.stack([concated_hidden_states[:,start_position], concated_hidden_states[:,end_position-1]]), 0)\n",
    "\t\t\t\t\n",
    "\t\t\tembedding = torch.cat([embedding, sentence_embedding], 0)\n",
    "\t\t\tif torch.isnan(embedding).any():\n",
    "\t\t\t\tprint(sentence)\n",
    "\t\t\t\tprint(tokenized_text)\n",
    "\t\t\t\tprint(sentence_embedding)\n",
    "\t\t\t\treturn\n",
    "\n",
    "\t\tsum_embedding = torch.sum(embedding, 0)\n",
    "\n",
    "\t\tnorm_embedding = torch.nn.functional.normalize(sum_embedding, dim=0)\n",
    "\t\tnorm_embedding = torch.unsqueeze(norm_embedding, dim=-1)\n",
    "\t\tnorm_embedding = torch.transpose(norm_embedding, 0, 1)\n",
    "\n",
    "\n",
    "\t\tembeddings = torch.cat([embeddings, norm_embedding], 0)\n",
    "\n",
    "\treturn embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_bert_embeddings_gender(sentence_list):\n",
    "\t#init for stacking embeddings\n",
    "\tembeddings = torch.empty(0, device=device)\n",
    "\n",
    "\tfor sentences in tqdm(sentence_list):\n",
    "\t\tembedding = torch.empty(0, device=device)\n",
    "\t\tfor i, sentence in enumerate(sentences):\n",
    "\t\t\t# print(f\"[{sentence}]\")\n",
    "\t\t\t# Map the token strings to their vocabulary indeces.\n",
    "\t\t\tmarked_text = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "\t\t\ttokenized_text = tokenizer.tokenize(marked_text)\n",
    "\t\t\t# handling such as \"wedding_dress\"\n",
    "\t\t\ttokenized_text = [token for token in tokenized_text if token != '_']\n",
    "\n",
    "\t\t\t# Split the sentence into tokens.\n",
    "\t\t\tindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\t\t\tsegments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "\t\t\t# Convert inputs to PyTorch tensors\n",
    "\t\t\ttokens_tensor = torch.tensor([indexed_tokens], device=device)\n",
    "\t\t\tsegments_tensors = torch.tensor([segments_ids], device=device)\n",
    "\n",
    "\t\t\t# Put the model in \"evaluation\" mode,meaning feed-forward operation.\n",
    "\t\t\tmodel.eval()\n",
    "\n",
    "\t\t\t# Run the text through BERT, get the output and collect all of the hidden states produced from all 12 layers.\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\toutputs = model(tokens_tensor, segments_tensors).hidden_states\n",
    "\t\t\n",
    "\t\t\t\tgender_position=-1\n",
    "\t\t\t\tfor idx, t in enumerate(tokenized_text):\n",
    "\t\t\t\t\tif t in gender_target_list:\n",
    "\t\t\t\t\t\tgender_position=idx\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif gender_position==-1:\n",
    "\t\t\t\t\tprint(\"error\")\n",
    "\t\t\t\t\tprint(tokenized_text)\n",
    "\t\t\t\t\treturn \n",
    "\n",
    "\t\t\t\n",
    "\t\t\t\tlast_four_hidden_states = outputs[-4:]\n",
    "\t\t\t\tconcated_hidden_states = torch.cat(last_four_hidden_states, dim=2)\n",
    "\t\t\t\tsentence_embedding = torch.squeeze(concated_hidden_states)[gender_position]\n",
    "\t\t\t\tsentence_embedding = torch.unsqueeze(sentence_embedding, dim=0)\n",
    "    \n",
    "\t\t\t\t\n",
    "\t\t\tembedding = torch.cat([embedding, sentence_embedding], 0)\n",
    "\t\n",
    "\t\t\tif torch.isnan(embedding).any():\n",
    "\t\t\t\tprint(sentence)\n",
    "\t\t\t\tprint(tokenized_text)\n",
    "\t\t\t\tprint(sentence_embedding)\n",
    "\t\t\t\treturn\n",
    "\n",
    "\n",
    "\t\tsum_embedding = torch.sum(embedding, 0)\n",
    "\t\tnorm_embedding = torch.nn.functional.normalize(sum_embedding, dim=0)\n",
    "\t\tnorm_embedding = torch.unsqueeze(norm_embedding, dim=0)\n",
    "\t\t# norm_embedding = torch.transpose(norm_embedding, 0, 1)\n",
    "\n",
    "\t\tembeddings = torch.cat([embeddings, norm_embedding], 1)\n",
    "\n",
    "\treturn embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# computing bert bias function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from torch import linalg as LA\n",
    "import scipy.stats\n",
    "import json \n",
    "import codecs\n",
    "\n",
    "# normalize vectors\n",
    "def normalize(wv):    \n",
    "    norms = LA.norm(wv, dim=1)\n",
    "    wv = wv / norms[:, np.newaxis]\n",
    "    return wv\n",
    "\n",
    "# compute bias from bert with he-she\n",
    "def compute_bias_by_projection_sentence(vocab, lim_wv, gender_word_embedding):\n",
    "    # print(gender_word_embedding[0].shape)\n",
    "    print(lim_wv)\n",
    "    males = torch.tensordot(lim_wv, gender_word_embedding[0], dims=1)\n",
    "    females = torch.tensordot(lim_wv, gender_word_embedding[1], dims=1)\n",
    "    \n",
    "    d = {}\n",
    "    for w, m, f in tqdm(zip(vocab, males, females)):\n",
    "        d[w] = m - f\n",
    "    return d\n",
    "\n",
    "def extract_professions():\n",
    "    professions = []\n",
    "    with codecs.open('../data/lists/professions.json', 'r', 'utf-8') as f:\n",
    "        professions_data = json.load(f)\n",
    "    for item in professions_data:\n",
    "        professions.append(item[0].strip())\n",
    "    return professions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Embeddings: 2016 words + bert + sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26189/26189 [00:00<00:00, 2623155.28it/s]\n",
      "100%|██████████| 18445/18445 [00:01<00:00, 10241.73it/s]\n",
      "100%|██████████| 18445/18445 [22:05<00:00, 13.91it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = open_pklfile(\"../data/extracted/0. original/original_word_2016_restricted_vocab.pkl\", 0)\n",
    "w2i, nouns, sentence_list = generate_noun_sentences(vocab)\n",
    "lim_wv = extract_bert_embeddings(sentence_list)\n",
    "dump_pklfile(nouns, f\"{save_path}2016_restricted_vocab.pkl\", 0)\n",
    "dump_pklfile(lim_wv, f\"{save_path}2016_restricted_embeddings.pkl\", 0)\n",
    "lim_wv = normalize(lim_wv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18445, 3072])\n",
      "tensor([[ 0.0199, -0.0264,  0.0297,  ..., -0.0131,  0.0085, -0.0414],\n",
      "        [ 0.0035, -0.0201,  0.0216,  ..., -0.0122,  0.0085,  0.0075],\n",
      "        [ 0.0238,  0.0003,  0.0193,  ..., -0.0204,  0.0008, -0.0056],\n",
      "        ...,\n",
      "        [ 0.0528, -0.0175,  0.0194,  ..., -0.0136, -0.0009,  0.0010],\n",
      "        [ 0.0377, -0.0423,  0.0124,  ..., -0.0163,  0.0119,  0.0038],\n",
      "        [ 0.0262, -0.0439,  0.0295,  ..., -0.0116,  0.0130, -0.0101]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18445it [00:00, 162419.07it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/lists/male_sentence_file.txt\", 'r') as f:\n",
    "    male_sentences = [sentence.strip() for sentence in f.readlines()]\n",
    "\n",
    "with open(\"../data/lists/female_sentence_file.txt\", 'r') as f:\n",
    "    female_sentences = [sentence.strip() for sentence in f.readlines()]\n",
    "\n",
    "male_embeddings = extract_bert_embeddings_gender([male_sentences])\n",
    "female_embeddings = extract_bert_embeddings_gender([female_sentences])\n",
    "# # print(male_embeddings.shape)\n",
    "male_embeddings = normalize(male_embeddings)\n",
    "female_embeddings = normalize(female_embeddings)\n",
    "\n",
    "lim_wv= normalize(open_pklfile(\"../data/extracted_positional/1. bert/bert_sentence_2016_restricted_embeddings.pkl\", 0))\n",
    "print(lim_wv.shape)\n",
    "\n",
    "gender_word_embedding = torch.cat((male_embeddings, female_embeddings), 0)\n",
    "\n",
    "\n",
    "\n",
    "gender_bias_all = compute_bias_by_projection_sentence(nouns, lim_wv, gender_word_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "sorted_g = sorted(gender_bias_all.items(), key=operator.itemgetter(1))\n",
    "females_w = [item[0] for item in sorted_g[:2500]]\n",
    "females_e = torch.empty(0, device=device)\n",
    "for w in females_w:\n",
    "    temp = torch.unsqueeze(lim_wv[w2i[w]], 0)\n",
    "    females_e = torch.cat([females_e, temp], dim=0)\n",
    "\n",
    "dump_pklfile(females_w, f\"{save_path}2016_female_2500_vocab.pkl\", 2500)\n",
    "dump_pklfile(females_e, f\"{save_path}2016_female_2500_embeddings.pkl\", 2500)\n",
    "\n",
    "sorted_g = sorted(gender_bias_all.items(), key=operator.itemgetter(1), reverse=True)\n",
    "males_w = [item[0] for item in sorted_g[:2500]]\n",
    "males_e = torch.empty(0, device=device)\n",
    "for w in males_w:\n",
    "    temp = torch.unsqueeze(lim_wv[w2i[w]], 0)\n",
    "    males_e = torch.cat([males_e, temp], dim=0)\n",
    "\n",
    "dump_pklfile(males_w, f\"{save_path}2016_male_2500_vocab.pkl\", 2500)\n",
    "dump_pklfile(males_e, f\"{save_path}2016_male_2500_embeddings.pkl\", 2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Embeddings: 2018 words + bert + sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47698/47698 [00:00<00:00, 2698771.24it/s]\n",
      "100%|██████████| 39385/39385 [00:03<00:00, 10638.02it/s]\n",
      "100%|██████████| 39385/39385 [49:49<00:00, 13.18it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = open_pklfile(\"../data/extracted/0. original/original_word_2018_restricted_vocab.pkl\", 0)\n",
    "w2i, nouns, sentence_list = generate_noun_sentences(vocab)\n",
    "lim_wv = extract_bert_embeddings(sentence_list)\n",
    "dump_pklfile(nouns, f\"{save_path}2018_restricted_vocab.pkl\", 0)\n",
    "dump_pklfile(lim_wv, f\"{save_path}2018_restricted_embeddings.pkl\", 0)\n",
    "\n",
    "lim_wv = normalize(lim_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0368e-02, -1.0730e-02,  1.5928e-03,  ..., -7.4427e-03,\n",
      "          1.5947e-03,  1.5316e-02],\n",
      "        [ 3.7644e-02,  4.4513e-03, -7.6410e-04,  ..., -1.6967e-02,\n",
      "          1.1548e-02,  1.8938e-04],\n",
      "        [-1.7022e-04, -3.0128e-02,  2.7313e-02,  ..., -1.1867e-02,\n",
      "          1.6743e-05, -1.0438e-02],\n",
      "        ...,\n",
      "        [ 1.7373e-02, -1.0948e-02, -2.6629e-03,  ..., -2.1898e-02,\n",
      "          4.6303e-03, -1.9816e-02],\n",
      "        [ 9.9961e-03, -1.7308e-02,  3.7393e-04,  ..., -2.2310e-02,\n",
      "          4.2690e-03,  1.5567e-02],\n",
      "        [ 1.3339e-02, -1.1929e-03,  7.9271e-03,  ..., -1.2481e-02,\n",
      "          1.5983e-02,  2.8010e-02]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39385it [00:00, 163960.66it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/lists/male_sentence_file.txt\", 'r') as f:\n",
    "  male_sentences = [sentence.strip() for sentence in f.readlines()]\n",
    "\n",
    "with open(\"../data/lists/female_sentence_file.txt\", 'r') as f:\n",
    "  female_sentences = [sentence.strip() for sentence in f.readlines()]\n",
    "\n",
    "male_embeddings = extract_bert_embeddings_gender([male_sentences])\n",
    "female_embeddings = extract_bert_embeddings_gender([female_sentences])\n",
    "\n",
    "male_embeddings = normalize(male_embeddings)\n",
    "female_embeddings = normalize(female_embeddings)\n",
    "\n",
    "gender_word_embedding = torch.cat((male_embeddings, female_embeddings), 0)\n",
    "\n",
    "gender_bias_all = compute_bias_by_projection_sentence(nouns, lim_wv, gender_word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "sorted_g = sorted(gender_bias_all.items(), key=operator.itemgetter(1))\n",
    "females_w = [item[0] for item in sorted_g[:2500]]\n",
    "females_e = torch.empty(0, device=device)\n",
    "\n",
    "for w in females_w:\n",
    "    temp = torch.unsqueeze(lim_wv[w2i[w]], 0)\n",
    "    females_e = torch.cat([females_e, temp], dim=0)\n",
    "\n",
    "dump_pklfile(females_w, f\"{save_path}2018_female_2500_vocab.pkl\", 2500)\n",
    "dump_pklfile(females_e, f\"{save_path}2018_female_2500_embeddings.pkl\", 2500)\n",
    "\n",
    "sorted_g = sorted(gender_bias_all.items(), key=operator.itemgetter(1), reverse=True)\n",
    "males_w = [item[0] for item in sorted_g[:2500]]\n",
    "males_e = torch.empty(0, device=device)\n",
    "\n",
    "for w in males_w:\n",
    "    temp = torch.unsqueeze(lim_wv[w2i[w]], 0)\n",
    "    males_e = torch.cat([males_e, temp], dim=0)\n",
    "\n",
    "dump_pklfile(males_w, f\"{save_path}2018_male_2500_vocab.pkl\", 2500)\n",
    "dump_pklfile(males_e, f\"{save_path}2018_male_2500_embeddings.pkl\", 2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract word file embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/lists/male_sentence_file.txt\", 'r') as f:\n",
    "  male_sentences = [sentence.strip() for sentence in f.readlines()]\n",
    "male_sentence_embs = extract_bert_embeddings_gender([male_sentences])\n",
    "dump_pklfile(male_sentence_embs, f\"{save_path}male_word_file_embeddings.pkl\", 0)\n",
    "\n",
    "with open(\"../data/lists/female_sentence_file.txt\", 'r') as f:\n",
    "  female_sentences = [sentence.strip() for sentence in f.readlines()]\n",
    "female_sentence_embs = extract_bert_embeddings_gender([female_sentences])\n",
    "dump_pklfile(female_sentence_embs, f\"{save_path}female_word_file_embeddings.pkl\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('exp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99b3c50fdb7e583f08a0b5d13592b9772c4dba729469d25211555e7350dc92e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
