{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases  in Word Embeddings But do not Remove Them\n",
    "\n",
    "For a detailed explanation of the experiments in this notebook, see:\n",
    "[paper](https://arxiv.org/pdf/1903.03862.pdf \"Lipstick on a Pig paper\")\n",
    "\n",
    "This notebook uses the debiased embeddings described in Zhao et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def load_embeddings_from_np(filename):\n",
    "    print('loading ...')\n",
    "    with codecs.open(filename + '.vocab', 'r', 'utf-8') as f_embed:\n",
    "        vocab = [line.strip() for line in f_embed]\n",
    "        \n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    wv = np.load(filename + '.wv.npy')\n",
    "\n",
    "    return vocab, wv, w2i\n",
    "\n",
    "\n",
    "def normalize(wv):\n",
    "    \n",
    "    # normalize vectors\n",
    "    norms = np.apply_along_axis(LA.norm, 1, wv)\n",
    "    wv = wv / norms[:, np.newaxis]\n",
    "    return wv\n",
    "\n",
    "\n",
    "def load_and_normalize(space, filename, vocab, wv, w2i):\n",
    "    vocab_muse, wv_muse, w2i_muse = load_embeddings_from_np(filename)\n",
    "    wv_muse = normalize(wv_muse)\n",
    "    vocab[space] = vocab_muse \n",
    "    wv[space] = wv_muse\n",
    "    w2i[space] = w2i_muse\n",
    "    print('done')\n",
    "    \n",
    "\n",
    "def load_wo_normalize(space, filename, vocab, wv, w2i):\n",
    "    vocab_muse, wv_muse, w2i_muse = load_embeddings_from_np(filename)\n",
    "    vocab[space] = vocab_muse \n",
    "    wv[space] = wv_muse\n",
    "    w2i[space] = w2i_muse\n",
    "    print('done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "vocab = {}\n",
    "wv = {}\n",
    "w2i = {}\n",
    "\n",
    "load_and_normalize('bef', '../data/embeddings/orig_glove', vocab, wv, w2i)\n",
    "load_wo_normalize('aft', '../data/embeddings/gn_glove', vocab, wv, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the last coordinate from the embedding and normalize\n",
    "\n",
    "for v in wv['aft']:\n",
    "    assert(len(v) == 300)\n",
    "wv['aft'] = wv['aft'][:,:-1]\n",
    "\n",
    "for v in wv['aft']:\n",
    "    assert(len(v) == 299)\n",
    "    \n",
    "wv['aft'] = normalize(wv['aft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def topK(w, space, k=10):\n",
    "    \n",
    "    # extract the word vector for word w\n",
    "    idx = w2i[space][w]\n",
    "    vec = wv[space][idx, :]\n",
    "    \n",
    "    # compute similarity of w with all words in the vocabulary\n",
    "    sim = wv[space].dot(vec)\n",
    "    # sort similarities by descending order\n",
    "    sort_sim = (sim.argsort())[::-1]\n",
    "\n",
    "    # choose topK\n",
    "    best = sort_sim[:(k+1)]\n",
    "\n",
    "    return [vocab[space][i] for i in best if i!=idx]\n",
    "\n",
    "\n",
    "def similarity(w1, w2, space):\n",
    "    \n",
    "    i1 = w2i[space][w1]\n",
    "    i2 = w2i[space][w2]\n",
    "    vec1 = wv[space][i1, :]\n",
    "    vec2 = wv[space][i2, :]\n",
    "\n",
    "    return np.inner(vec1,vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "\n",
    "def has_punct(w):\n",
    "    \n",
    "    if any([c in string.punctuation for c in w]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def has_digit(w):\n",
    "    \n",
    "    if any([c in '0123456789' for c in w]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def limit_vocab(space, exclude = None, vec_len = 300):\n",
    "    vocab_limited = []\n",
    "    for w in tqdm(vocab[space][:50000]): \n",
    "        if w.lower() != w:\n",
    "            continue\n",
    "        if len(w) >= 20:\n",
    "            continue\n",
    "        if has_digit(w):\n",
    "            continue\n",
    "        if '_' in w:\n",
    "            p = [has_punct(subw) for subw in w.split('_')]\n",
    "            if not any(p):\n",
    "                vocab_limited.append(w)\n",
    "            continue\n",
    "        if has_punct(w):\n",
    "            continue\n",
    "        vocab_limited.append(w)\n",
    "    \n",
    "    if exclude:\n",
    "        vocab_limited = list(set(vocab_limited) - set(exclude))\n",
    "    \n",
    "    print( \"size of vocabulary:\", len(vocab_limited))\n",
    "    \n",
    "    wv_limited = np.zeros((len(vocab_limited), vec_len))\n",
    "    for i,w in enumerate(vocab_limited):\n",
    "        wv_limited[i,:] = wv[space][w2i[space][w],:]\n",
    "    \n",
    "    w2i_limited = {w: i for i, w in enumerate(vocab_limited)}\n",
    "    \n",
    "    return vocab_limited, wv_limited, w2i_limited\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the reduced vocabularies and embeddings before and after, without gendered specific words\n",
    "\n",
    "gender_specific = []\n",
    "with open('../data/lists/male_word_file.txt') as f:\n",
    "    for l in f:\n",
    "        gender_specific.append(l.strip())\n",
    "with open('../data/lists/female_word_file.txt') as f:\n",
    "    for l in f:\n",
    "        gender_specific.append(l.strip())\n",
    "\n",
    "exclude_words = gender_specific\n",
    "\n",
    "# create spaces of limited vocabulary\n",
    "vocab['limit_bef'], wv['limit_bef'], w2i['limit_bef'] = limit_vocab('bef', exclude = exclude_words)\n",
    "vocab['limit_aft'], wv['limit_aft'], w2i['limit_aft'] = limit_vocab('aft', exclude = exclude_words, vec_len = 299)\n",
    "\n",
    "assert(vocab['limit_aft'] == vocab['limit_bef'])\n",
    "\n",
    "print(len(vocab['limit_bef']))\n",
    "\n",
    "with open('../data/extracted/0. original/original_word_2018_restricted_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab['limit_bef'], f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute bias-by-projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of the bias, before and after\n",
    "\n",
    "def compute_bias_by_projection(space_to_tag, full_space):\n",
    "    males = wv[space_to_tag].dot(wv[full_space][w2i[full_space]['he'],:])\n",
    "    females = wv[space_to_tag].dot(wv[full_space][w2i[full_space]['she'],:])\n",
    "    d = {}\n",
    "    for w,m,f in zip(vocab[space_to_tag], males, females):\n",
    "        d[w] = m-f\n",
    "    return d\n",
    "\n",
    "# compute bias-by-projection before and after debiasing\n",
    "gender_bias_bef = compute_bias_by_projection('limit_bef', 'bef')\n",
    "gender_bias_aft = compute_bias_by_projection('limit_aft', 'aft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the avg bias of the vocabulary (abs) before and after debiasing\n",
    "\n",
    "def report_bias(gender_bias):\n",
    "    bias = 0.0\n",
    "    for k in gender_bias:\n",
    "        bias += np.abs(gender_bias[k])\n",
    "    print(bias/len(gender_bias))\n",
    "report_bias(gender_bias_bef)\n",
    "report_bias(gender_bias_aft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Coreelation between bias-by-projection and bias-by-neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tuples of biases and counts of masculine/feminine NN for each word (for bias-by-neighbors)\n",
    "\n",
    "def bias_by_neighbors(space, neighbours_num = 100):\n",
    "    \n",
    "    tuples = []\n",
    "    for w in tqdm(vocab[space]):\n",
    "        \n",
    "        top = topK(w, space, k=neighbours_num+5)[:neighbours_num]\n",
    "\n",
    "        m = 0\n",
    "        f = 0    \n",
    "        for t in top:\n",
    "            if gender_bias_bef[t] > 0:\n",
    "                m+=1\n",
    "            else:\n",
    "                f+=1\n",
    "            \n",
    "        tuples.append((w, gender_bias_bef[w], gender_bias_aft[w], m, f))\n",
    "\n",
    "    return tuples\n",
    "        \n",
    "tuples_bef = bias_by_neighbors('limit_bef') \n",
    "tuples_aft = bias_by_neighbors('limit_aft')       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation between bias-by-projection and bias-by-neighbors\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "def pearson(a,b):\n",
    "   \n",
    "    return scipy.stats.pearsonr(a,b)\n",
    "\n",
    "def compute_corr(tuples, i1, i2):\n",
    "    \n",
    "    a = []\n",
    "    b = []\n",
    "    for t in tuples:\n",
    "        a.append(t[i1])\n",
    "        b.append(t[i2])\n",
    "    assert(len(a)==len(b))    \n",
    "    print (pearson(a,b))\n",
    "\n",
    "compute_corr(tuples_bef, 1, 3)\n",
    "compute_corr(tuples_aft, 1, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Experiment - Visualize clusters of most biased words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary finctions\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "from cycler import cycler\n",
    "%matplotlib inline\n",
    "mpld3.enable_notebook()\n",
    "mpl.rc(\"savefig\", dpi=200)\n",
    "mpl.rcParams['figure.figsize'] = (8,8)\n",
    "mpl.rcParams['axes.prop_cycle'] = cycler(color='rc')\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "#from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize(vectors, words, labels, ax, title, random_state, num_clusters = 2):\n",
    "    \n",
    "    # perform TSNE\n",
    "    \n",
    "    X_embedded = TSNE(n_components=2, random_state=random_state).fit_transform(vectors)\n",
    "    if num_clusters == 2:\n",
    "        for x,l in zip(X_embedded, labels):\n",
    "            if l:\n",
    "                ax.scatter(x[0], x[1], marker = '.', c = 'c')\n",
    "            else:\n",
    "                ax.scatter(x[0], x[1], marker = 'x', c = 'darkviolet')\n",
    "    else:\n",
    "        ax.scatter(X_embedded[:,0], X_embedded[:,1], c = labels)                \n",
    "    \n",
    "    ax.text(.01, .9, title ,transform=ax.transAxes, fontsize=18)\n",
    "\n",
    "    \n",
    "def extract_vectors(words, space1 = 'limit_bef', space2 = 'limit_aft'):\n",
    "    \n",
    "    size = len(words)/2\n",
    "    \n",
    "    X_bef = [wv[space1][w2i[space1][x],:] for x in words]\n",
    "    X_aft = [wv[space2][w2i[space2][x],:] for x in words]\n",
    "\n",
    "    return X_bef, X_aft\n",
    "\n",
    "\n",
    "def cluster_and_visualize(words, X_bef, X_aft, random_state, y_true, num=2):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n",
    "    \n",
    "    y_pred_bef = KMeans(n_clusters=num, random_state=random_state).fit_predict(X_bef)\n",
    "    visualize(X_bef, words, y_pred_bef, axs[0], 'Original', random_state)\n",
    "    correct = [1 if item1 == item2 else 0 for (item1,item2) in zip(y_true, y_pred_bef) ]\n",
    "    print('precision bef', sum(correct)/float(len(correct)))\n",
    "    \n",
    "    y_pred_aft = KMeans(n_clusters=num, random_state=random_state).fit_predict(X_aft)\n",
    "    visualize(X_aft, words, y_pred_aft, axs[1], 'Debiased', random_state)\n",
    "    correct = [1 if item1 == item2 else 0 for (item1,item2) in zip(y_true, y_pred_aft) ]\n",
    "    print('precision aft', sum(correct)/float(len(correct)))\n",
    "    fig.show()\n",
    "    fig.savefig(\"../results/2018_clustering figure\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster most biased words before and after debiasing\n",
    "import operator\n",
    "import random\n",
    "random.seed(1)\n",
    "random_state = 1\n",
    "\n",
    "size = 500\n",
    "sorted_g = sorted(gender_bias_bef.items(), key=operator.itemgetter(1))\n",
    "female = [item[0] for item in sorted_g[:size]]\n",
    "male = [item[0] for item in sorted_g[-size:]]\n",
    "X_bef, X_aft = extract_vectors(male + female)\n",
    "y_true = [0]*size + [1]*size\n",
    "cluster_and_visualize(male + female, X_bef, X_aft, random_state, y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Professions experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def extract_professions():\n",
    "    professions = []\n",
    "    with codecs.open('../data/lists/professions.json', 'r', 'utf-8') as f:\n",
    "        professions_data = json.load(f)\n",
    "    for item in professions_data:\n",
    "        professions.append(item[0].strip())\n",
    "    return professions\n",
    "\n",
    "\n",
    "professions = extract_professions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def get_tuples_prof(space, words, gender_bias_dict):\n",
    "\n",
    "    tuples = []\n",
    "    for w in words:\n",
    "        if w not in gender_bias_dict:\n",
    "            continue\n",
    "            \n",
    "        top = topK(w, space, k=105)[:100]\n",
    "            \n",
    "        m = 0\n",
    "        f = 0  \n",
    "        for t in top:          \n",
    "            if gender_bias_dict[t] > 0:\n",
    "                m+=1\n",
    "            else:\n",
    "                f+=1\n",
    "                \n",
    "        tuples.append((w, gender_bias_bef[w], gender_bias_aft[w], m, f))\n",
    "        \n",
    "    return tuples\n",
    "\n",
    "\n",
    "tuples_bef_prof = get_tuples_prof('limit_bef', professions, gender_bias_bef)\n",
    "tuples_aft_prof = get_tuples_prof('limit_aft', professions, gender_bias_bef)\n",
    "\n",
    "compute_corr(tuples_bef_prof, 1, 3)\n",
    "compute_corr(tuples_aft_prof, 1, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plots\n",
    "\n",
    "def show_plots(tuples_bef_prof, tuples_aft_prof):\n",
    "    \n",
    "    fig, axs = plt.subplots(2,1, figsize=(8,8))\n",
    "    \n",
    "    for i,(tuples, title) in enumerate(zip([tuples_bef_prof, tuples_aft_prof], ['Original', 'Debiased'])):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for t in tuples:\n",
    "            X.append(t[1])\n",
    "            Y.append(t[3])\n",
    "\n",
    "        axs[i].scatter(X,Y, color = 'c', s=12)\n",
    "        axs[i].set_ylim(0,100)\n",
    "        \n",
    "        for t in tuples:\n",
    "            if t[0] in ['nanny', 'dancer', 'housekeeper', 'receptionist', 'nurse',\\\n",
    "                   'magician', 'musician', 'warden', 'archaeologist', 'comic', 'dentist', \\\n",
    "                    'inventor', 'colonel', 'farmer', 'skipper', 'commander', 'coach']:\n",
    "                axs[i].annotate(t[0], xy=(t[1], t[3]), xytext=(t[1], t[3]), textcoords=\"data\", fontsize=12) \n",
    "        axs[i].text(.03, .85, title, transform=axs[i].transAxes, fontsize=20)\n",
    "    \n",
    "    \n",
    "    fig.show()\n",
    "    fig.savefig(\"../results/2018_professions figure\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "show_plots(tuples_bef_prof, tuples_aft_prof)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 5000 most biased words, split each polarity randomly to train (1/5) and test (4/5), and predict\n",
    "\n",
    "from sklearn import svm\n",
    "from random import shuffle\n",
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "\n",
    "\n",
    "def train_and_predict(space_train, space_test):\n",
    "    \n",
    "    X_train = [wv[space_train][w2i[space_train][w],:] for w in males[:size_train]+females[:size_train]]\n",
    "    Y_train = [1]*size_train + [0]*size_train\n",
    "    X_test = [wv[space_test][w2i[space_test][w],:] for w in males[size_train:]+females[size_train:]]\n",
    "    Y_test = [1]*size_test + [0]*size_test\n",
    "\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    print('train with', space_train)\n",
    "    print('test with', space_test)\n",
    "\n",
    "    preds = clf.predict(X_test)\n",
    "\n",
    "    accuracy = [1 if y==z else 0 for y,z in zip(preds, Y_test)]\n",
    "    print('accuracy:', float(sum(accuracy))/len(accuracy))\n",
    "\n",
    "    \n",
    "# extract nost biased words\n",
    "\n",
    "size_train = 500\n",
    "size_test = 2000\n",
    "size = size_train + size_test\n",
    "sorted_g = sorted(gender_bias_bef.items(), key=operator.itemgetter(1))\n",
    "females = [item[0] for item in sorted_g[:size]]\n",
    "males = [item[0] for item in sorted_g[-size:]]\n",
    "\n",
    "with open('../data/extracted/0. original/original_word_2018_male_2500.pkl', 'wb') as f:\n",
    "    pickle.dump(females, f)\n",
    "    \n",
    "with open('../data/extracted/0. original/original_word_2018_female_2500.pkl', 'wb') as f:\n",
    "    pickle.dump(males, f)\n",
    "\n",
    "for f in females:\n",
    "    assert(gender_bias_bef[f] < 0)\n",
    "for m in males:\n",
    "    assert(gender_bias_bef[m] > 0)\n",
    "shuffle(females)\n",
    "shuffle(males)\n",
    "\n",
    "# classification before debiasing\n",
    "\n",
    "train_and_predict('bef', 'bef')\n",
    "\n",
    "# classification after debiasing\n",
    "\n",
    "train_and_predict('aft', 'aft')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pig",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea0dbe1ab80c28379ee8d63d29c942d972d2acb15d80d02181912983ca222734"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
