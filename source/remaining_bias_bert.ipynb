{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install requiremenets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install pickle5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained Bert model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open files and extract embedding from bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "\n",
    "def open_pklfile(filepath, size):\n",
    "\twith open(filepath, \"rb\") as f:\n",
    "\t\tif (size == 0):\n",
    "\t\t\treturn pickle.load(f)\n",
    "\t\treturn (pickle.load(f))[0:size]\n",
    "\n",
    "def extract_bert_embeddings(word_list):\n",
    "\t#init for stacking embeddings\n",
    "\tembeddings = torch.empty(0)\n",
    "\tfor word in word_list:\n",
    "\t\t\n",
    "\t\tif (word in '___'):\n",
    "\t\t\tcontinue\n",
    "\t\t# Map the token strings to their vocabulary indeces.\n",
    "\t\ttokenized_text = tokenizer.tokenize(word)\n",
    "\t\t\n",
    "\t\t# handling such as \"wedding_dress\"\n",
    "\t\ttokenized_text = [token for token in tokenized_text if token != '_']\n",
    "\n",
    "\t\t# Split the sentence into tokens.\n",
    "\t\tindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "\t\t# Convert inputs to PyTorch tensors\n",
    "\t\ttokens_tensor = torch.tensor([indexed_tokens])\n",
    "\t\t\n",
    "\t\t# Put the model in \"evaluation\" mode,meaning feed-forward operation.\n",
    "\t\tmodel.eval()\n",
    "\n",
    "\t\t#Run the text through BERT, get the output and collect all of the hidden states produced from all 12 layers.\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = model(tokens_tensor)\n",
    "\t\t\t# can use last hidden state as word embeddings\n",
    "\t\t\tlast_hidden_state = outputs[0]\n",
    "\t\t\tlast_hidden_state = torch.sum(last_hidden_state, 1, True)\n",
    "\t\t\tembeddings = torch.cat([embeddings, last_hidden_state], dim = 1)\n",
    "\n",
    "\tprint(embeddings.shape)\n",
    "\treturn torch.squeeze(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Bias computing with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 : Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary finctions\n",
    "\n",
    "import matplotlib as mpl\n",
    "import mpld3\n",
    "from cycler import cycler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import sys\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "#from sklearn.datasets import make_blobs\n",
    "\n",
    "%matplotlib inline\n",
    "mpld3.enable_notebook()\n",
    "mpl.rc(\"savefig\", dpi=200)\n",
    "mpl.rcParams['figure.figsize'] = (8,8)\n",
    "mpl.rcParams['axes.prop_cycle'] = cycler(color='rc')\n",
    "\n",
    "\n",
    "def visualize(vectors, words, labels, ax, title, random_state, num_clusters = 2):\n",
    "    \n",
    "    # perform TSNE\n",
    "    X_embedded = TSNE(n_components=2, random_state=random_state).fit_transform(vectors)\n",
    "    if num_clusters == 2:\n",
    "        for x,l in zip(X_embedded, labels):\n",
    "            if l:\n",
    "                ax.scatter(x[0], x[1], marker = '.', c = 'c')\n",
    "            else:\n",
    "                ax.scatter(x[0], x[1], marker = 'x', c = 'darkviolet')\n",
    "    else:\n",
    "        ax.scatter(X_embedded[:,0], X_embedded[:,1], c = labels)                \n",
    "    \n",
    "    ax.text(.01, .9, title ,transform=ax.transAxes, fontsize=18)\n",
    "\n",
    "    \n",
    "def extract_vectors(words, space1 = 'limit_bef', space2 = 'limit_aft'):\n",
    "    \n",
    "    size = len(words)/2\n",
    "    \n",
    "    X_bef = [wv[space1][w2i[space1][x],:] for x in words]\n",
    "    X_aft = [wv[space2][w2i[space2][x],:] for x in words]\n",
    "\n",
    "    return X_bef, X_aft\n",
    "\n",
    "\n",
    "def cluster_and_visualize(words, X_bef, X_aft, random_state, y_true, num=2):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n",
    "    \n",
    "    y_pred_bef = KMeans(n_clusters=num, random_state=random_state).fit_predict(X_bef)\n",
    "    visualize(X_bef, words, y_pred_bef, axs[0], 'Original', random_state)\n",
    "    correct = [1 if item1 == item2 else 0 for (item1,item2) in zip(y_true, y_pred_bef) ]\n",
    "    print ('precision bef', sum(correct)/float(len(correct)))\n",
    "    \n",
    "    y_pred_aft = KMeans(n_clusters=num, random_state=random_state).fit_predict(X_aft)\n",
    "    visualize(X_aft, words, y_pred_aft, axs[1], 'Debiased', random_state)\n",
    "    correct = [1 if item1 == item2 else 0 for (item1,item2) in zip(y_true, y_pred_aft) ]\n",
    "    print ('precision aft', sum(correct)/float(len(correct)))\n",
    "    fig.show()\n",
    "    fig.savefig(\"2016_clustering figure\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster most biased words before and after debiasing\n",
    "import operator\n",
    "\n",
    "size = 500\n",
    "top500_male_words = open_pklfile(\"../data/lists/top2500male_2016.pkl\", 500)\n",
    "top500_male_embeddings = extract_bert_embeddings(top500_male_words)\n",
    "\n",
    "top500_female_words = open_pklfile(\"../data/lists/top2500female_2016.pkl\", 500)\n",
    "top500_fe_embeddings = extract_bert_embeddings(top500_female_words)\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "# sorting with bert bias\n",
    "# sorted_g = sorted(gender_bias_bef.items(), key=operator.itemgetter(1))\n",
    "# female = [item[0] for item in sorted_g[:size]]\n",
    "# male = [item[0] for item in sorted_g[-size:]]\n",
    "\n",
    "# X_bef, X_aft = extract_vectors(male + female)\n",
    "y_true = [1]*size + [0]*size\n",
    "cluster_and_visualize(top500_male_words + top500_female_words,\n",
    " top500_male_embeddings + top500_fe_embeddings,\n",
    "  top500_male_embeddings + top500_fe_embeddings,\n",
    "   random_state, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Experiment 2 : Professions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3 : Classification with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 2500\n",
    "top2500_male_words = open_pklfile(\"../data/lists/top2500male_2016.pkl\", 0)\n",
    "top2500_male_embeddings = extract_bert_embeddings(top2500_male_words)\n",
    "\n",
    "top2500_female_words = open_pklfile(\"../data/lists/top2500female_2016.pkl\", 0)\n",
    "top2500_fe_embeddings = extract_bert_embeddings(top2500_female_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 5000 most biased words, split each polarity randomly to train (1/5) and test (4/5), and predict\n",
    "\n",
    "from sklearn import svm\n",
    "from random import shuffle\n",
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "def train_and_predict(male_embeddings, female_embeddings):\n",
    "    size_train = 500\n",
    "    size_test = 1999\n",
    "\n",
    "    # X_train = [wv[space_train][w2i[space_train][w],:] for w in males[:size_train]+females[:size_train]]\n",
    "    # Y_train = [1]*size_train + [0]*size_train\n",
    "    # X_test = [wv[space_test][w2i[space_test][w],:] for w in males[size_train:]+females[size_train:]]\n",
    "    # Y_test = [1]*size_test + [0]*size_test\n",
    "\n",
    "    total = size_train+size_test\n",
    "    X_train = torch.cat([male_embeddings[:size_train], female_embeddings[:size_train]], dim = 0)\n",
    "    Y_train = [1]*size_train + [0]*size_train\n",
    "\n",
    "    X_test = torch.cat([male_embeddings[size_train:total], female_embeddings[size_train:total]], dim = 0)\n",
    "    Y_test = [1]*size_test + [0]*size_test\n",
    "\n",
    "    print(\"train set shape is \", X_train.shape)\n",
    "    print(\"test set shape is \", X_test.shape)\n",
    "    clf = svm.SVC()\n",
    "\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    preds = clf.predict(X_test)\n",
    "\n",
    "    accuracy = [1 if y==z else 0 for y,z in zip(preds, Y_test)]\n",
    "    print ('accuracy:', float(sum(accuracy))/len(accuracy))\n",
    "\n",
    "# size = size_train + size_test\n",
    "# sorted_g = sorted(gender_bias_bef.items(), key=operator.itemgetter(1))\n",
    "# females = [item[0] for item in sorted_g[:size]]\n",
    "# males = [item[0] for item in sorted_g[-size:]]\n",
    "# for f in females:\n",
    "#     assert(gender_bias_bef[f] < 0)\n",
    "# for m in males:\n",
    "#     assert(gender_bias_bef[m] > 0)\n",
    "\n",
    "\n",
    "shuffle(top2500_male_embeddings)\n",
    "shuffle(top2500_fe_embeddings)\n",
    "\n",
    "# classification before debiasing\n",
    "\n",
    "train_and_predict(top2500_male_embeddings, top2500_fe_embeddings)\n",
    "\n",
    "# classification after debiasing\n",
    "\n",
    "# train_and_predict('aft', 'aft')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea0dbe1ab80c28379ee8d63d29c942d972d2acb15d80d02181912983ca222734"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
