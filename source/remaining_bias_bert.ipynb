{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install requiremenets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install pickle5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained Bert model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open files and extract embedding from bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "\n",
    "def open_pklfile(filepath, size):\n",
    "\twith open(filepath, \"rb\") as f:\n",
    "\t\tif (size == 0):\n",
    "\t\t\treturn pickle.load(f)\n",
    "\t\treturn (pickle.load(f))[0:size]\n",
    "\n",
    "def extract_bert_embeddings(word_list):\n",
    "\t#init for stacking embeddings\n",
    "\tembeddings = torch.empty(0)\n",
    "\tfor word in word_list:\n",
    "\t\t\n",
    "\t\tif (word in '___'):\n",
    "\t\t\tcontinue\n",
    "\t\t# Map the token strings to their vocabulary indeces.\n",
    "\t\tmarked_text = \"[CLS] \" + word + \" [SEP]\"\n",
    "\t\ttokenized_text = tokenizer.tokenize(marked_text)\n",
    "\t\t\n",
    "\t\t# handling such as \"wedding_dress\"\n",
    "\t\ttokenized_text = [token for token in tokenized_text if token != '_']\n",
    "\n",
    "\t\t# Split the sentence into tokens.\n",
    "\t\tindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\t\tsegments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "\t\t# Convert inputs to PyTorch tensors\n",
    "\t\ttokens_tensor = torch.tensor([indexed_tokens])\n",
    "\t\tsegments_tensors = torch.tensor([segments_ids])\n",
    "\t\t\n",
    "\t\t# Put the model in \"evaluation\" mode,meaning feed-forward operation.\n",
    "\t\tmodel.eval()\n",
    "\n",
    "\t\t#Run the text through BERT, get the output and collect all of the hidden states produced from all 12 layers.\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = model(tokens_tensor, segments_tensors)\n",
    "\t\t\t\n",
    "\t\t\tlast_four_hidden_states = outputs[2][-4:]\n",
    "\t\t\tconcated_hidden_states = torch.cat(last_four_hidden_states, dim = 2)\n",
    "\t\t\t\n",
    "\t\t\tfirst_last = torch.add(concated_hidden_states[:, 1], concated_hidden_states[:, -2])\n",
    "\t\t\tembeddings = torch.cat([embeddings, first_last])\n",
    "\n",
    "\tprint(embeddings.shape)\n",
    "\treturn torch.squeeze(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Bias computing with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 : Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary finctions\n",
    "\n",
    "import matplotlib as mpl\n",
    "import mpld3\n",
    "from cycler import cycler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import sys\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "#from sklearn.datasets import make_blobs\n",
    "\n",
    "%matplotlib inline\n",
    "mpld3.enable_notebook()\n",
    "mpl.rc(\"savefig\", dpi=200)\n",
    "mpl.rcParams['figure.figsize'] = (8,8)\n",
    "mpl.rcParams['axes.prop_cycle'] = cycler(color='rc')\n",
    "\n",
    "\n",
    "def visualize(vectors, words, labels, ax, title, random_state, num_clusters = 2):\n",
    "    \n",
    "    # perform TSNE\n",
    "    X_embedded = TSNE(n_components=2, random_state=random_state).fit_transform(vectors)\n",
    "    if num_clusters == 2:\n",
    "        for x,l in zip(X_embedded, labels):\n",
    "            if l:\n",
    "                ax.scatter(x[0], x[1], marker = '.', c = 'c')\n",
    "            else:\n",
    "                ax.scatter(x[0], x[1], marker = 'x', c = 'darkviolet')\n",
    "    else:\n",
    "        ax.scatter(X_embedded[:,0], X_embedded[:,1], c = labels)                \n",
    "    \n",
    "    ax.text(.01, .9, title ,transform=ax.transAxes, fontsize=18)\n",
    "\n",
    "    \n",
    "def extract_vectors(words, space1 = 'limit_bef', space2 = 'limit_aft'):\n",
    "    \n",
    "    size = len(words)/2\n",
    "    \n",
    "    X_bef = [wv[space1][w2i[space1][x],:] for x in words]\n",
    "    X_aft = [wv[space2][w2i[space2][x],:] for x in words]\n",
    "\n",
    "    return X_bef, X_aft\n",
    "\n",
    "\n",
    "def cluster_and_visualize(words, X_bef, X_aft, random_state, y_true, save_filename, num=2):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n",
    "    \n",
    "    y_pred_bef = KMeans(n_clusters=num, random_state=random_state).fit_predict(X_bef)\n",
    "    visualize(X_bef, words, y_pred_bef, axs[0], 'Original', random_state)\n",
    "    correct = [1 if item1 == item2 else 0 for (item1,item2) in zip(y_true, y_pred_bef) ]\n",
    "    print ('precision bef', sum(correct)/float(len(correct)))\n",
    "    \n",
    "    y_pred_aft = KMeans(n_clusters=num, random_state=random_state).fit_predict(X_aft)\n",
    "    visualize(X_aft, words, y_pred_aft, axs[1], 'Debiased', random_state)\n",
    "    correct = [1 if item1 == item2 else 0 for (item1,item2) in zip(y_true, y_pred_aft) ]\n",
    "    print ('precision aft', sum(correct)/float(len(correct)))\n",
    "    fig.show()\n",
    "    fig.savefig(save_filename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016 Hard_Debiased Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016 Hard_debiased\n",
    "# Cluster most biased words before and after debiasing\n",
    "import operator\n",
    "\n",
    "size = 500\n",
    "top500_male_words = open_pklfile(\"../data/lists/top2500male_2016.pkl\", 500)\n",
    "top500_male_embeddings = extract_bert_embeddings(top500_male_words)\n",
    "\n",
    "top500_female_words = open_pklfile(\"../data/lists/top2500female_2016.pkl\", 500)\n",
    "top500_fe_embeddings = extract_bert_embeddings(top500_female_words)\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "# sorting with bert bias\n",
    "# sorted_g = sorted(gender_bias_bef.items(), key=operator.itemgetter(1))\n",
    "# female = [item[0] for item in sorted_g[:size]]\n",
    "# male = [item[0] for item in sorted_g[-size:]]\n",
    "\n",
    "# X_bef, X_aft = extract_vectors(male + female)\n",
    "y_true = [1]*size + [0]*size\n",
    "cluster_and_visualize(top500_male_words + top500_female_words,\n",
    " top500_male_embeddings + top500_fe_embeddings,\n",
    "  top500_male_embeddings + top500_fe_embeddings,\n",
    "   random_state, y_true, \"bert2016_clustering_figure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2018 GN_GloVe Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018 GN_GloVe\n",
    "# Cluster most biased words before and after debiasing\n",
    "import operator\n",
    "\n",
    "size = 500\n",
    "top500_male_words = open_pklfile(\"../data/lists/top2500male_2018.pkl\", 500)\n",
    "top500_male_embeddings = extract_bert_embeddings(top500_male_words)\n",
    "\n",
    "top500_female_words = open_pklfile(\"../data/lists/top2500female_2018.pkl\", 500)\n",
    "top500_fe_embeddings = extract_bert_embeddings(top500_female_words)\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "# sorting with bert bias\n",
    "# sorted_g = sorted(gender_bias_bef.items(), key=operator.itemgetter(1))\n",
    "# female = [item[0] for item in sorted_g[:size]]\n",
    "# male = [item[0] for item in sorted_g[-size:]]\n",
    "\n",
    "# X_bef, X_aft = extract_vectors(male + female)\n",
    "y_true = [1]*size + [0]*size\n",
    "cluster_and_visualize(top500_male_words + top500_female_words,\n",
    " top500_male_embeddings + top500_fe_embeddings,\n",
    "  top500_male_embeddings + top500_fe_embeddings,\n",
    "   random_state, y_true, \"bert2018_clustering_figure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Experiment 2 : Professions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3 : Classification with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016 Hard_Debiased SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016 Hard_debiased\n",
    "\n",
    "size = 2500\n",
    "top2500_male_words = open_pklfile(\"../data/lists/top2500male_2016.pkl\", 0)\n",
    "top2500_male_embeddings = extract_bert_embeddings(top2500_male_words)\n",
    "\n",
    "top2500_female_words = open_pklfile(\"../data/lists/top2500female_2016.pkl\", 0)\n",
    "top2500_fe_embeddings = extract_bert_embeddings(top2500_female_words)\n",
    "\n",
    "# size = size_train + size_test\n",
    "# sorted_g = sorted(gender_bias_bef.items(), key=operator.itemgetter(1))\n",
    "# females = [item[0] for item in sorted_g[:size]]\n",
    "# males = [item[0] for item in sorted_g[-size:]]\n",
    "# for f in females:\n",
    "#     assert(gender_bias_bef[f] < 0)\n",
    "# for m in males:\n",
    "#     assert(gender_bias_bef[m] > 0)\n",
    "\n",
    "shuffle(top2500_male_embeddings)\n",
    "shuffle(top2500_fe_embeddings)\n",
    "\n",
    "# classification before debiasing\n",
    "\n",
    "train_and_predict(top2500_male_embeddings, top2500_fe_embeddings)\n",
    "\n",
    "# classification after debiasing\n",
    "\n",
    "# train_and_predict('aft', 'aft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2018 GN_GloVe SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018 GN_GloVe\n",
    "\n",
    "size = 2500\n",
    "top2500_male_words = open_pklfile(\"../data/lists/top2500male_2018.pkl\", 0)\n",
    "top2500_male_embeddings = extract_bert_embeddings(top2500_male_words)\n",
    "\n",
    "top2500_female_words = open_pklfile(\"../data/lists/top2500female_2018.pkl\", 0)\n",
    "top2500_fe_embeddings = extract_bert_embeddings(top2500_female_words)\n",
    "\n",
    "# size = size_train + size_test\n",
    "# sorted_g = sorted(gender_bias_bef.items(), key=operator.itemgetter(1))\n",
    "# females = [item[0] for item in sorted_g[:size]]\n",
    "# males = [item[0] for item in sorted_g[-size:]]\n",
    "# for f in females:\n",
    "#     assert(gender_bias_bef[f] < 0)\n",
    "# for m in males:\n",
    "#     assert(gender_bias_bef[m] > 0)\n",
    "\n",
    "shuffle(top2500_male_embeddings)\n",
    "shuffle(top2500_fe_embeddings)\n",
    "\n",
    "# classification before debiasing\n",
    "\n",
    "train_and_predict(top2500_male_embeddings, top2500_fe_embeddings)\n",
    "\n",
    "# classification after debiasing\n",
    "\n",
    "# train_and_predict('aft', 'aft')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea0dbe1ab80c28379ee8d63d29c942d972d2acb15d80d02181912983ca222734"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
