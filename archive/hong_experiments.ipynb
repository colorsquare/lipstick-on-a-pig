{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install pickle5\n",
    "%pip install mpld3\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debias Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"bert\", \"1. bert\" if not using a debiased model\n",
    "# debiased_model = \"bert\"\n",
    "# debiased_folder = \"1. bert\"\n",
    "# image_caption = \"Bert\"\n",
    "\n",
    "# debiased_model = \"sent_debiased\"\n",
    "# debiased_folder = \"2. sent_debiased\"\n",
    "# image_caption = \"Sent Debiased\"\n",
    "\n",
    "# debiased_model = \"contextualised\"\n",
    "# debiased_folder = \"3. contextualised\"\n",
    "# image_caption = \"Contextualised\"\n",
    "\n",
    "debiased_model = \"cds\"\n",
    "debiased_folder = \"4. cds\"\n",
    "image_caption = \"CDS\"\n",
    "\n",
    "prefix = \"../data/extracted\"\n",
    "\n",
    "is_sentence = True\n",
    "word_or_sent = \"word\" if not is_sentence else \"sentence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load words and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle5 as pickle\n",
    "\n",
    "def open_pklfile(filepath, size):\n",
    "\t\"\"\"Open pickle file from the file path.\n",
    "\n",
    "\tLoad all contents if `size` is 0, else slice and return.\n",
    "\t\"\"\"\n",
    "\twith open(filepath, \"rb\") as f:\n",
    "\t\tdata = pickle.load(f)\n",
    "\treturn data if not size else data[:size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 : Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "from cycler import cycler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "mpld3.enable_notebook()\n",
    "mpl.rc(\"savefig\", dpi=200)\n",
    "mpl.rcParams['figure.figsize'] = (8,8)\n",
    "mpl.rcParams['axes.prop_cycle'] = cycler(color='rc')\n",
    "\n",
    "\n",
    "def visualize(embeddings, labels, ax, title, random_state, num_clusters = 2):\n",
    "    X_embedded = TSNE(n_components=2, random_state=random_state).fit_transform(embeddings)\n",
    "    if num_clusters == 2:\n",
    "        for x, l in zip(X_embedded, labels):\n",
    "            if l:\n",
    "                ax.scatter(x[0], x[1], marker = '.', c = 'c')\n",
    "            else:\n",
    "                ax.scatter(x[0], x[1], marker = 'x', c = 'darkviolet')\n",
    "    else:\n",
    "        ax.scatter(X_embedded[:,0], X_embedded[:,1], c = labels)\n",
    "    ax.text(.01, .9, title ,transform=ax.transAxes, fontsize=18)\n",
    "\n",
    "\n",
    "def cluster_and_visualize(embed_bef, embed_aft, labels, save_filename, random_state = 1, num=2):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n",
    "\n",
    "    # Clustering embeddings before debiasing\n",
    "    prediction_bef = KMeans(n_clusters=num, random_state=random_state).fit_predict(embed_bef.cpu())\n",
    "    visualize(embed_bef.cpu(), labels, axs[0], \"Bert\", random_state)\n",
    "    correct = [1 if item1 == item2 else 0 for (item1, item2) in zip(labels, prediction_bef)]\n",
    "    print(\"Precision before: \", sum(correct)/len(correct))\n",
    "    \n",
    "    # Clustering embeddings after debiasing\n",
    "    prediction_aft = KMeans(n_clusters=num, random_state=random_state).fit_predict(embed_aft.cpu())\n",
    "    visualize(embed_aft.cpu(), labels, axs[1], image_caption, random_state)\n",
    "    correct = [1 if item1 == item2 else 0 for (item1, item2) in zip(labels, prediction_aft)]\n",
    "    print(\"Precision after: \", sum(correct)/len(correct))\n",
    "    \n",
    "    fig.show()\n",
    "    fig.savefig(\"../results/\" + save_filename, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016 Hard_Debiased Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016 Hard_debiased\n",
    "# Cluster most biased words before and after debiasing\n",
    "\n",
    "# debiased model with bert_words\n",
    "debiased_top500_male_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_2016_male_2500_embeddings.pkl\", 500)\n",
    "debiased_top500_female_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_2016_female_2500_embeddings.pkl\", 500)\n",
    "\n",
    "# debiased model with debiased_words\n",
    "hong_debiased_top500_male_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_hong2016_male_2500_embeddings.pkl\", 500)\n",
    "hong_debiased_top500_female_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_hong2016_female_2500_embeddings.pkl\", 500)\n",
    "\n",
    "embeddings = torch.cat([debiased_top500_male_embeddings, debiased_top500_female_embeddings], dim=0)\n",
    "debiased_embeddings = torch.cat([hong_debiased_top500_male_embeddings, hong_debiased_top500_female_embeddings], dim=0)\n",
    "\n",
    "labels = [0] * 500 + [1] * 500  # 0 for male, 1 for female\n",
    "cluster_and_visualize(embeddings, debiased_embeddings, labels, f\"{debiased_folder}/{debiased_model}_{word_or_sent}_clustering_2016\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2018 GN_GloVe Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018 GN_GloVe\n",
    "# Cluster most biased words before and after debiasing\n",
    "\n",
    "# Bert\n",
    "debiased_top500_male_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_2018_male_2500_embeddings.pkl\", 500)\n",
    "debiased_top500_female_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_2018_female_2500_embeddings.pkl\", 500)\n",
    "\n",
    "# Debiased\n",
    "hong_debiased_top500_male_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_hong2018_male_2500_embeddings.pkl\", 500)\n",
    "hong_debiased_top500_female_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_hong2018_female_2500_embeddings.pkl\", 500)\n",
    "\n",
    "embeddings = torch.cat([debiased_top500_male_embeddings, debiased_top500_female_embeddings], dim=0)\n",
    "debiased_embeddings = torch.cat([hong_debiased_top500_male_embeddings, hong_debiased_top500_female_embeddings], dim=0)\n",
    "\n",
    "labels = [0] * 500 + [1] * 500  # 0 for male, 1 for female\n",
    "cluster_and_visualize(embeddings, debiased_embeddings, labels, f\"{debiased_folder}/{debiased_model}_{word_or_sent}_hongclustering_2018\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "def train_and_predict(male_embeddings, female_embeddings):\n",
    "    assert(len(male_embeddings) == len(female_embeddings))\n",
    "    assert(len(male_embeddings) % 5 == 0)\n",
    "    total = len(male_embeddings)\n",
    "    train_size, test_size = int(total * 0.2), int(total * 0.8)\n",
    "    \n",
    "    # Train\n",
    "    train_embed = torch.cat([male_embeddings[:train_size], female_embeddings[:train_size]], dim = 0)\n",
    "    train_label = [1] * train_size + [0] * train_size\n",
    "\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(train_embed.cpu(), train_label)\n",
    "\n",
    "    # Test\n",
    "    test_embed = torch.cat([male_embeddings[train_size:total], female_embeddings[train_size:total]], dim = 0)\n",
    "    test_label = [1] * test_size + [0] * test_size\n",
    "\n",
    "    predictions = clf.predict(test_embed.cpu())\n",
    "    accuracy = [1 if pred == label else 0 for pred, label in zip(predictions, test_label)]\n",
    "    print(\"Classification accuracy: \", sum(accuracy) / len(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016 Hard_debiased\n",
    "# Classify 2500 most biased words before and after debiasing\n",
    "\n",
    "# debiased model with bert_words\n",
    "debiased_top500_male_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_2016_male_2500_embeddings.pkl\", 0)\n",
    "debiased_top500_female_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_2016_female_2500_embeddings.pkl\", 0)\n",
    "\n",
    "# debiased model with debiased_words\n",
    "hong_debiased_top500_male_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_hong2016_male_2500_embeddings.pkl\", 0)\n",
    "hong_debiased_top500_female_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_hong2016_female_2500_embeddings.pkl\", 0)\n",
    "\n",
    "# Bert\n",
    "random.shuffle(debiased_top500_male_embeddings)\n",
    "random.shuffle(debiased_top500_female_embeddings)\n",
    "train_and_predict(debiased_top500_male_embeddings, debiased_top500_female_embeddings)\n",
    "\n",
    "# Debiased\n",
    "random.shuffle(hong_debiased_top500_male_embeddings)\n",
    "random.shuffle(hong_debiased_top500_female_embeddings)\n",
    "train_and_predict(hong_debiased_top500_male_embeddings, hong_debiased_top500_female_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018 GN_GloVe\n",
    "# Classify 2500 most biased words before and after debiasing\n",
    "\n",
    "# debiased model with bert_words\n",
    "debiased_top500_male_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_2018_male_2500_embeddings.pkl\", 0)\n",
    "debiased_top500_female_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_2018_female_2500_embeddings.pkl\", 0)\n",
    "\n",
    "# debiased model with debiased_words\n",
    "hong_debiased_top500_male_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_hong2018_male_2500_embeddings.pkl\", 0)\n",
    "hong_debiased_top500_female_embeddings = open_pklfile(f\"{prefix}/{debiased_folder}/{debiased_model}_{word_or_sent}_hong2018_female_2500_embeddings.pkl\", 0)\n",
    "\n",
    "# Bert\n",
    "random.shuffle(debiased_top500_male_embeddings)\n",
    "random.shuffle(debiased_top500_female_embeddings)\n",
    "train_and_predict(debiased_top500_male_embeddings, debiased_top500_female_embeddings)\n",
    "\n",
    "# Debiased\n",
    "random.shuffle(hong_debiased_top500_male_embeddings)\n",
    "random.shuffle(hong_debiased_top500_female_embeddings)\n",
    "train_and_predict(hong_debiased_top500_male_embeddings, hong_debiased_top500_female_embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('exp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99b3c50fdb7e583f08a0b5d13592b9772c4dba729469d25211555e7350dc92e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
